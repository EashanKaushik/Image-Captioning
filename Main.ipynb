{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numpy import array\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport string\nimport os\nfrom PIL import Image\nimport glob\nimport pickle\nfrom pickle import dump, load\nfrom time import time\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras import Input, layers\nfrom keras import optimizers\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"filename = \"../input/flickrtext/Flickr8k.token.txt\"\n\ndef load_doc(filename):\n    file = open(filename, 'r')\n    text = file.read()\n    file.close()\n    return text\n\ndoc = load_doc(filename)\nprint(doc[:300])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_descriptions(doc):\n    mapping = dict()\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        if len(line) < 2:\n            continue\n        image_id, image_desc = tokens[0], tokens[1:]\n        image_id = image_id.split('.')[0]\n        image_desc = ' '.join(image_desc)\n        if image_id not in mapping:\n            mapping[image_id] = list()\n        mapping[image_id].append(image_desc)\n    return mapping\n\ndescriptions = load_descriptions(doc)\nprint('Loaded: %d ' % len(descriptions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(descriptions.keys())[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"descriptions['1000268201_693b08cb0e']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_descriptions(descriptions):\n    table = str.maketrans('', '', string.punctuation)\n    for key, desc_list in descriptions.items():\n        for i in range(len(desc_list)):\n            desc = desc_list[i]\n            desc = desc.split()\n            desc = [word.lower() for word in desc]\n            desc = [w.translate(table) for w in desc]\n            desc = [word for word in desc if len(word)>1]\n            desc = [word for word in desc if word.isalpha()]\n            desc_list[i] =  ' '.join(desc)\n\nclean_descriptions(descriptions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"descriptions['1000268201_693b08cb0e']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_vocabulary(descriptions):\n    all_desc = set()\n    for key in descriptions.keys():\n        [all_desc.update(d.split()) for d in descriptions[key]]\n    return all_desc\n\nvocabulary = to_vocabulary(descriptions)\nprint('Original Vocabulary Size: %d' % len(vocabulary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_descriptions(descriptions, filename):\n    lines = list()\n    for key, desc_list in descriptions.items():\n        for desc in desc_list:\n            lines.append(key + ' ' + desc)\n    data = '\\n'.join(lines)\n    file = open(filename, 'w')\n    file.write(data)\n    file.close()\n\nsave_descriptions(descriptions, 'descriptions.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_set(filename):\n    doc = load_doc(filename)\n    dataset = list()\n    for line in doc.split('\\n'):\n        if len(line) < 1:\n            continue\n        identifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)\n\nfilename = '../input/flickrtext/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = '../input/flickr8k/Images/'\nimg = glob.glob(images + '*.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_file = '../input/flickrtext/Flickr_8k.trainImages.txt'\ntrain_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n\ntrain_img = []\n\nfor i in img: \n    if i[len(images):] in train_images: \n        train_img.append(i) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images_file = '../input/flickrtext/Flickr_8k.testImages.txt'\ntest_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n\ntest_img = []\n\nfor i in img: \n    if i[len(images):] in test_images: \n        test_img.append(i) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_clean_descriptions(filename, dataset):\n    doc = load_doc(filename)\n    descriptions = dict()\n    for line in doc.split('\\n'):\n        tokens = line.split()\n        image_id, image_desc = tokens[0], tokens[1:]\n        if image_id in dataset:\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n            descriptions[image_id].append(desc)\n    return descriptions\n\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = InceptionV3(weights='imagenet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_new = Model(model.input, model.layers[-2].output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(image):\n    image = preprocess(image) \n    fea_vec = model_new.predict(image) \n    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) \n    return fea_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time()\nencoding_train = {}\nfor img in train_img:\n    encoding_train[img[len(images):]] = encode(img)\nprint(\"Time taken in seconds =\", time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n    pickle.dump(encoding_train, encoded_pickle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time()\nencoding_test = {}\nfor img in test_img:\n    encoding_test[img[len(images):]] = encode(img)\nprint(\"Time taken in seconds =\", time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n    pickle.dump(encoding_test, encoded_pickle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = load(open(\"/kaggle/working/encoded_train_images.pkl\", \"rb\"))\nprint('Photos: train=%d' % len(train_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_train_captions = []\nfor key, val in train_descriptions.items():\n    for cap in val:\n        all_train_captions.append(cap)\nlen(all_train_captions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count_threshold = 10\nword_counts = {}\nnsents = 0\nfor sent in all_train_captions:\n    nsents += 1\n    for w in sent.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\n\nvocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\nprint('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ixtoword = {}\nwordtoix = {}\n\nix = 1\nfor w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(ixtoword) + 1 # one for appended 0's\nvocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\ndef max_length(descriptions):\n    lines = to_lines(descriptions)\n    return max(len(d.split()) for d in lines)\n\nmax_length = max_length(train_descriptions)\nprint('Description Length: %d' % max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n    X1, X2, y = list(), list(), list()\n    n=0\n    while 1:\n        for key, desc_list in descriptions.items():\n            n+=1\n            photo = photos[key+'.jpg']\n            for desc in desc_list:\n                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    X1.append(photo)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            if n==num_photos_per_batch:\n                yield [[array(X1), array(X2)], array(y)]\n                X1, X2, y = list(), list(), list()\n                n=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {} \nf = open('../input/flickrtext/glove.6B.200d.txt', encoding=\"utf-8\")\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 200\n\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in wordtoix.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# epochs = 20\n# number_pics_per_bath = 3\n# steps = len(train_descriptions)//number_pics_per_bath","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(epochs):\n#     generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n#     model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n# model.save('model_1.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# epochs = 10\n# number_pics_per_bath = 6\n# steps = len(train_descriptions)//number_pics_per_bath","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(epochs):\n#     generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n#     model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n# model.save('model_1.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.save_weights('model_f.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.load_weights('/kaggle/working/model_f.h5')\nmodel.load_weights('/kaggle/input/ml-projectmodel/model_f.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = '../input/flickr8k/Images/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"/kaggle/working/encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n    encoding_test = load(encoded_pickle)\n    print(encoding_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def greedySearch(photo):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = ixtoword[yhat]\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n    final = in_text.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n    return final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z=1\npic = list(encoding_test.keys())[z]\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images+pic)\nplt.imshow(x)\nplt.show()\nprint(\"Caption:\",greedySearch(image))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}